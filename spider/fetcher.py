import os
import json
import requests
import mimetypes
import time
import random
import re
from datetime import datetime
from bs4 import BeautifulSoup
from urllib.parse import urljoin, unquote
import urllib3
from playwright.sync_api import sync_playwright, Error as PlaywrightError

# ç¦ç”¨ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# è·¯å¾„é…ç½®
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_DIR = os.path.join(BASE_DIR, "data")
COOKIE_FILE = os.path.join(DATA_DIR, "cookies.json")
STATE_FILE = os.path.join(DATA_DIR, "state.json")
TEMP_DIR = os.path.join(DATA_DIR, "temp_files")

DEFAULT_USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

# ==========================================
# ğŸ”§ å·¥å…·å‡½æ•°
# ==========================================

def _get_playwright_cookies(context):
    """ä» Playwright ä¸Šä¸‹æ–‡ä¸­æå– Cookie ç»™ requests ç”¨"""
    cookies = context.cookies()
    cookie_dict = {}
    for c in cookies:
        cookie_dict[c['name']] = c['value']
    return cookie_dict

def get_filename_from_cd(cd):
    """ä» Content-Disposition å¤´ä¸­æå–æ–‡ä»¶å"""
    if not cd:
        return None
    # å°è¯•æå– filename="xxx" æˆ– filename*=utf-8''xxx
    fname = re.findall(r'filename="?([^"]+)"?', cd)
    if not fname:
        fname = re.findall(r"filename\*=utf-8''(.+)", cd)

    if fname:
        return unquote(fname[0]) # è§£ç  URL ç¼–ç çš„æ–‡ä»¶å
    return None

def download_file(url, cookie_dict, suggested_name=None):
    """
    æ™ºèƒ½ä¸‹è½½æ–‡ä»¶
    1. æ”¯æŒä» Header è·å–çœŸå®æ–‡ä»¶å
    2. æ”¯æŒè‡ªåŠ¨æ¨æ–­åç¼€
    """
    if not os.path.exists(TEMP_DIR):
        os.makedirs(TEMP_DIR)

    try:
        print(f"    â¬‡ï¸ æ­£åœ¨è¯·æ±‚é™„ä»¶é“¾æ¥...")
        session = requests.Session()
        session.headers.update({"User-Agent": DEFAULT_USER_AGENT})
        session.cookies.update(cookie_dict)

        # å¢åŠ è¶…æ—¶ï¼Œæµå¼ä¸‹è½½
        res = session.get(url, stream=True, verify=False, timeout=60)

        # ğŸŸ¢ [æ ¸å¿ƒå‡çº§] è·å–çœŸå®æ–‡ä»¶å
        final_filename = "unknown_file.dat"

        # 1. ä¼˜å…ˆå°è¯•ä»å“åº”å¤´è·å– (æœ€å‡†)
        server_filename = get_filename_from_cd(res.headers.get('Content-Disposition'))

        if server_filename:
            final_filename = server_filename
        elif suggested_name:
            # 2. å¦‚æœæ²¡ç»™ï¼Œç”¨é“¾æ¥æ–‡å­— (æ¯”å¦‚ "é™„ä»¶ï¼šxxx.doc")
            # æ¸…æ´—æ–‡ä»¶åï¼Œå»æ‰ "é™„ä»¶ï¼š" å’Œéæ³•å­—ç¬¦
            clean_name = re.sub(r'é™„ä»¶[ï¼š:]\s*', '', suggested_name).strip()
            clean_name = re.sub(r'[\\/*?:"<>|]', "", clean_name)
            if clean_name:
                final_filename = clean_name
                # å¦‚æœæ–‡å­—é‡Œæ²¡åç¼€ï¼Œå°è¯•è¡¥å…¨
                if '.' not in final_filename:
                    ct = res.headers.get('Content-Type', '').split(';')[0]
                    ext = mimetypes.guess_extension(ct)
                    if ext: final_filename += ext

        # æ„é€ ä¿å­˜è·¯å¾„
        save_path = os.path.join(TEMP_DIR, final_filename)

        # é¿å…è¦†ç›–
        if os.path.exists(save_path):
            name, ext = os.path.splitext(final_filename)
            final_filename = f"{name}_{int(time.time())}{ext}"
            save_path = os.path.join(TEMP_DIR, final_filename)

        with open(save_path, "wb") as f:
            for chunk in res.iter_content(chunk_size=8192):
                f.write(chunk)

        print(f"    âœ… é™„ä»¶ä¸‹è½½æˆåŠŸ: {final_filename}")
        return save_path
    except Exception as e:
        print(f"    âš ï¸ ä¸‹è½½å¤±è´¥: {e}")
        return None

# ==========================================
# ğŸ§± åŸå­ç»„ä»¶ï¼šè§£æé€»è¾‘
# ==========================================

def _extract_attachments(soup, base_url, cookie_dict):
    """
    æå–å¹¶ä¸‹è½½é™„ä»¶ (é’ˆå¯¹ JSP åŠ¨æ€é“¾æ¥ä¼˜åŒ–)
    """
    files = []
    # æå–å¸¸è§„é™„ä»¶é“¾æ¥
    for a in soup.find_all('a', href=True):
        href = a['href']
        text = a.get_text(strip=True)

        full_link = urljoin(base_url, href)
        lower_link = full_link.lower()

        # ğŸŸ¢ [æ ¸å¿ƒå‡çº§] åˆ¤å®šè§„åˆ™
        # è§„åˆ™1: ä¼ ç»Ÿçš„é™æ€æ–‡ä»¶åç¼€
        valid_exts = ['.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx', '.zip', '.rar']
        is_static_file = any(x in lower_link for x in valid_exts)

        # è§„åˆ™2: åŠ¨æ€ä¸‹è½½é“¾æ¥ (é’ˆå¯¹ä½ æä¾›çš„ download.jsp)
        # åªè¦é“¾æ¥é‡ŒåŒ…å« 'download.jsp' æˆ–è€… 'wbfileid'ï¼Œä¸ç®¡æœ‰æ²¡æœ‰åç¼€ï¼Œéƒ½ç®—é™„ä»¶ï¼
        is_dynamic_file = 'download.jsp' in lower_link or 'downloadattachurl' in lower_link or 'wbfileid' in lower_link

        if is_static_file or is_dynamic_file:
            # è¿‡æ»¤æ‰éæ–‡ä»¶é“¾æ¥ (å¦‚ mailto)
            if 'mailto:' in lower_link or 'javascript:' in lower_link: continue

            # ä¸‹è½½ (ä¼ å…¥é“¾æ¥æ–‡å­—ä½œä¸ºå¤‡é€‰æ–‡ä»¶å)
            f_path = download_file(full_link, cookie_dict, suggested_name=text)
            if f_path: files.append(f_path)

    return files

def _process_html(html_content, base_url, cookie_dict):
    """å¤„ç† HTML æ–‡æœ¬"""
    soup = BeautifulSoup(html_content, 'html.parser')

    # 1. æå–æ­£æ–‡
    text = soup.get_text(separator='\n', strip=True)

    # 2. æå–é™„ä»¶
    files = _extract_attachments(soup, base_url, cookie_dict)

    return {
        "type": "compound",
        "text": text[:8000],
        "files": files
    }

# ==========================================
# ğŸš€ ä¸»å…¥å£ (Retry + Stealth)
# ==========================================

def fetch_content(url):
    """
    ä¸»æŠ“å–å‡½æ•°ï¼šä½¿ç”¨ Playwright åŠ è½½é¡µé¢
    """
    MAX_RETRIES = 3

    for attempt in range(1, MAX_RETRIES + 1):
        try:
            if attempt > 1:
                wait_time = random.uniform(2, 5) * attempt
                print(f"    â³ ç½‘ç»œæ³¢åŠ¨ï¼Œç­‰å¾… {wait_time:.1f}s åç¬¬ {attempt} æ¬¡å°è¯•...")
                time.sleep(wait_time)

            with sync_playwright() as p:
                browser = p.chromium.launch(
                    headless=False, # ä¿æŒæœ‰å¤´æ¨¡å¼é˜²å°
                    args=['--disable-blink-features=AutomationControlled']
                )

                if os.path.exists(STATE_FILE):
                    context = browser.new_context(
                        storage_state=STATE_FILE,
                        user_agent=DEFAULT_USER_AGENT
                    )
                else:
                    context = browser.new_context(user_agent=DEFAULT_USER_AGENT)

                context.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

                page = context.new_page()

                try:
                    page.goto(url, timeout=90000, wait_until="domcontentloaded")
                except PlaywrightError as e:
                    if "ERR_EMPTY_RESPONSE" in str(e) or "ERR_CONNECTION_RESET" in str(e):
                        print(f"    âš ï¸ è¿æ¥è¢«åˆ‡æ–­ ({e.message.split(' at ')[0]})...")
                        browser.close()
                        continue
                    else:
                        raise e

                page.wait_for_timeout(3000) # å¤šç­‰ä¸€ä¼š

                if "404" in page.title() or "æŠ±æ­‰" in page.content():
                    print("    âŒ é¡µé¢ 404 (Stateå¯èƒ½å¤±æ•ˆ)")
                    browser.close()
                    return None

                if "login" in page.url:
                    print("    âŒ Cookie/State å·²å¤±æ•ˆ")
                    browser.close()
                    return None

                html = page.content()
                fresh_cookies = _get_playwright_cookies(context)
                browser.close()

                return _process_html(html, url, fresh_cookies)

        except Exception as e:
            print(f"    âŒ ç¬¬ {attempt} æ¬¡æŠ“å–å¤±è´¥: {e}")
            if attempt == MAX_RETRIES:
                return None

    return None