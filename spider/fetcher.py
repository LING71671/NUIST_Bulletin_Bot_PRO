import os
import json
import requests
import mimetypes
from datetime import datetime
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import urllib3
from playwright.sync_api import sync_playwright

# ç¦ç”¨ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# è·¯å¾„é…ç½®
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_DIR = os.path.join(BASE_DIR, "data")
COOKIE_FILE = os.path.join(DATA_DIR, "cookies.json")
STATE_FILE = os.path.join(DATA_DIR, "state.json") # ğŸŸ¢ å¿…é¡»åŠ è½½è¿™ä¸ªï¼
TEMP_DIR = os.path.join(DATA_DIR, "temp_files")

DEFAULT_USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

# ==========================================
# ğŸ”§ å·¥å…·å‡½æ•°
# ==========================================

def _get_playwright_cookies(context):
    """ä» Playwright ä¸Šä¸‹æ–‡ä¸­æå– Cookie ç»™ requests ç”¨"""
    cookies = context.cookies()
    cookie_dict = {}
    for c in cookies:
        cookie_dict[c['name']] = c['value']
    return cookie_dict

def download_file(url, cookie_dict):
    """ä½¿ç”¨ requests ä¸‹è½½æ–‡ä»¶"""
    if not os.path.exists(TEMP_DIR):
        os.makedirs(TEMP_DIR)

    try:
        print(f"    â¬‡ï¸ ä¸‹è½½é™„ä»¶: {url.split('/')[-1][:20]}...")
        session = requests.Session()
        session.headers.update({"User-Agent": DEFAULT_USER_AGENT})
        session.cookies.update(cookie_dict)

        res = session.get(url, stream=True, verify=False, timeout=60)

        content_type = res.headers.get('Content-Type', '').split(';')[0]
        ext = mimetypes.guess_extension(content_type) or ".dat"
        if '.' in url.split('/')[-1]:
            ext = '.' + url.split('/')[-1].split('.')[-1]

        filename = f"attach_{datetime.now().strftime('%H%M%S_%f')}{ext}"
        path = os.path.join(TEMP_DIR, filename)

        with open(path, "wb") as f:
            for chunk in res.iter_content(chunk_size=8192):
                f.write(chunk)
        return path
    except Exception as e:
        print(f"    âš ï¸ ä¸‹è½½å¤±è´¥: {e}")
        return None

# ==========================================
# ğŸ§± åŸå­ç»„ä»¶ï¼šè§£æé€»è¾‘
# ==========================================

def _extract_attachments(soup, base_url, cookie_dict):
    """æå–å¹¶ä¸‹è½½é™„ä»¶"""
    files = []
    # æå–å¸¸è§„é™„ä»¶é“¾æ¥
    for a in soup.find_all('a', href=True):
        href = a['href']
        full_link = urljoin(base_url, href)
        lower_link = full_link.lower()

        valid_exts = ['.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx', '.zip', '.rar']

        if any(x in lower_link for x in valid_exts):
            if 'mailto:' in lower_link or 'javascript:' in lower_link: continue
            f_path = download_file(full_link, cookie_dict)
            if f_path: files.append(f_path)

    return files

def _process_html(html_content, base_url, cookie_dict):
    """å¤„ç† HTML æ–‡æœ¬"""
    soup = BeautifulSoup(html_content, 'html.parser')

    # 1. æå–æ­£æ–‡
    text = soup.get_text(separator='\n', strip=True)

    # 2. æå–é™„ä»¶
    files = _extract_attachments(soup, base_url, cookie_dict)

    return {
        "type": "compound",
        "text": text[:8000], # å¢å¤§ä¸Šä¸‹æ–‡
        "files": files
    }

# ==========================================
# ğŸš€ ä¸»å…¥å£ (Playwright + Stateæ³¨å…¥ + éšèº«)
# ==========================================

def fetch_content(url):
    """
    ä¸»æŠ“å–å‡½æ•°ï¼šä½¿ç”¨ Playwright åŠ è½½é¡µé¢
    å·²åŠ å…¥åçˆ¬è™«å¯¹æŠ—å‚æ•°ï¼Œå¹¶å¼ºåˆ¶æ³¨å…¥ State
    """
    try:
        with sync_playwright() as p:
            # 1. å¯åŠ¨æµè§ˆå™¨ (å…³é—­æ— å¤´ï¼Œå¯ç”¨éšèº«)
            browser = p.chromium.launch(
                headless=False, # å¿…é¡»æœ‰å¤´ï¼Œå¦åˆ™ VPN ä¼šæ‹¦æˆª
                args=['--disable-blink-features=AutomationControlled']
            )

            # 2. ğŸŸ¢ æ ¸å¿ƒä¿®å¤ï¼šåŠ è½½å®Œæ•´çš„ State (Cookies + LocalStorage)
            if os.path.exists(STATE_FILE):
                # print(f"    ğŸ“‚ [Fetcher] åŠ è½½èº«ä»½å‡­è¯: {STATE_FILE}")
                context = browser.new_context(
                    storage_state=STATE_FILE,
                    user_agent=DEFAULT_USER_AGENT
                )
            else:
                print("    âš ï¸ ä¸¥é‡è­¦å‘Š: èº«ä»½å‡­è¯ä¸¢å¤±ï¼Œå¯èƒ½å¯¼è‡´ 404ï¼")
                context = browser.new_context(user_agent=DEFAULT_USER_AGENT)

            # 3. æ³¨å…¥é˜²æ£€æµ‹è„šæœ¬ (åŒé‡ä¿é™©)
            context.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

            page = context.new_page()

            # 4. è®¿é—®é¡µé¢
            # print(f"    ğŸ”— æ­£åœ¨åŠ è½½è¯¦æƒ…é¡µ...")
            page.goto(url, timeout=60000, wait_until="domcontentloaded")

            # 5. ç­‰å¾…å†…å®¹åŠ è½½ (é˜²æ­¢ç©ºç™½)
            page.wait_for_timeout(2000)

            # 6. æ£€æŸ¥æ˜¯å¦æ˜¯ 404 æˆ– ç™»å½•é¡µ
            title = page.title()
            if "404" in title or "æŠ±æ­‰" in page.content():
                print("    âŒ é¡µé¢ 404ï¼Œå¯èƒ½æ˜¯æƒé™ä¸è¶³æˆ– State å¤±æ•ˆ")
                browser.close()
                return None

            if "login" in page.url or "ç™»å½•" in title:
                print("    âŒ Cookie/State å·²å¤±æ•ˆï¼Œæ— æ³•æŠ“å–")
                browser.close()
                return None

            html = page.content()
            fresh_cookies = _get_playwright_cookies(context)
            browser.close()

            return _process_html(html, url, fresh_cookies)

    except Exception as e:
        print(f"    âŒ æŠ“å–å†…å®¹å‡ºé”™: {e}")
        return None