import os
import json
import requests
import mimetypes
import time
import random
import re
from datetime import datetime
from bs4 import BeautifulSoup
from urllib.parse import urljoin, unquote
import urllib3
from playwright.sync_api import sync_playwright, Error as PlaywrightError

# ç¦ç”¨ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# è·¯å¾„é…ç½®
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_DIR = os.path.join(BASE_DIR, "data")
COOKIE_FILE = os.path.join(DATA_DIR, "cookies.json")
STATE_FILE = os.path.join(DATA_DIR, "state.json")
TEMP_DIR = os.path.join(DATA_DIR, "temp_files")

DEFAULT_USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

# ==========================================
# ğŸ”§ å·¥å…·å‡½æ•°
# ==========================================

def _get_playwright_cookies(context):
    """ä» Playwright ä¸Šä¸‹æ–‡ä¸­æå– Cookie"""
    cookies = context.cookies()
    cookie_dict = {}
    for c in cookies:
        cookie_dict[c['name']] = c['value']
    return cookie_dict

def sanitize_filename(name):
    """æ¸…æ´—æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
    if not name: return None
    name = unquote(name)
    name = re.sub(r'[\\/*?:"<>|;]', "", name)
    name = name.replace("\n", "").replace("\r", "").strip()
    return name[:200]

def get_filename_from_cd(cd):
    """ä» Content-Disposition å¤´ä¸­æå–æ–‡ä»¶å"""
    if not cd: return None

    # 1. ä¼˜å…ˆå°è¯• filename*=utf-8''xxx
    fname_utf8 = re.search(r"filename\*=utf-8''([^;]+)", cd, re.IGNORECASE)
    if fname_utf8:
        return unquote(fname_utf8.group(1))

    # 2. å…¶æ¬¡å°è¯• filename="xxx"
    fname_quoted = re.search(r'filename="([^"]+)"', cd, re.IGNORECASE)
    if fname_quoted:
        name = fname_quoted.group(1)
        try: return name.encode('iso-8859-1').decode('utf-8')
        except: return name

    # 3. æœ€åå°è¯• filename=xxx
    fname_simple = re.search(r'filename=([^;]+)', cd, re.IGNORECASE)
    if fname_simple:
        name = fname_simple.group(1).strip()
        try: return name.encode('iso-8859-1').decode('utf-8')
        except: return name

    return None

def download_file(url, cookie_dict, suggested_name=None):
    """æ™ºèƒ½ä¸‹è½½æ–‡ä»¶"""
    if not os.path.exists(TEMP_DIR):
        os.makedirs(TEMP_DIR)

    try:
        print(f"    â¬‡ï¸ æ­£åœ¨è¯·æ±‚é™„ä»¶é“¾æ¥...")
        session = requests.Session()
        session.headers.update({"User-Agent": DEFAULT_USER_AGENT})
        session.cookies.update(cookie_dict)

        res = session.get(url, stream=True, verify=False, timeout=60)

        final_filename = "unknown.dat"
        server_filename = get_filename_from_cd(res.headers.get('Content-Disposition'))

        if server_filename:
            final_filename = server_filename
        elif suggested_name:
            base_name = suggested_name
            if '.' not in base_name:
                ct = res.headers.get('Content-Type', '').split(';')[0]
                ext = mimetypes.guess_extension(ct)
                if ext: base_name += ext
            final_filename = base_name

        final_filename = sanitize_filename(final_filename)
        if not final_filename:
            final_filename = f"attach_{int(time.time())}.dat"

        save_path = os.path.join(TEMP_DIR, final_filename)
        if os.path.exists(save_path):
            name, ext = os.path.splitext(final_filename)
            final_filename = f"{name}_{int(time.time())}{ext}"
            save_path = os.path.join(TEMP_DIR, final_filename)

        with open(save_path, "wb") as f:
            for chunk in res.iter_content(chunk_size=8192):
                f.write(chunk)

        print(f"    âœ… é™„ä»¶ä¸‹è½½æˆåŠŸ: {final_filename}")
        return save_path
    except Exception as e:
        print(f"    âš ï¸ ä¸‹è½½å¤±è´¥: {e}")
        return None

# ==========================================
# ğŸ§± åŸå­ç»„ä»¶ï¼šå†…å®¹è§£æ
# ==========================================

def _extract_attachments(soup, base_url, cookie_dict):
    """æå–é™„ä»¶é“¾æ¥"""
    files = []
    for a in soup.find_all('a', href=True):
        href = a['href']
        text = a.get_text(strip=True)
        full_link = urljoin(base_url, href)
        lower_link = full_link.lower()

        valid_exts = ['.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx', '.zip', '.rar']
        is_static = any(x in lower_link for x in valid_exts)
        is_dynamic = 'download.jsp' in lower_link or 'downloadattachurl' in lower_link or 'wbfileid' in lower_link

        if is_static or is_dynamic:
            if 'mailto:' in lower_link or 'javascript:' in lower_link: continue
            clean_text = re.sub(r'^é™„ä»¶[ï¼š:]\s*', '', text).strip()
            f_path = download_file(full_link, cookie_dict, suggested_name=clean_text)
            if f_path: files.append(f_path)
    return files

def _process_html(html_content, base_url, cookie_dict):
    soup = BeautifulSoup(html_content, 'html.parser')
    text = soup.get_text(separator='\n', strip=True)
    files = _extract_attachments(soup, base_url, cookie_dict)
    return {
        "type": "compound",
        "text": text[:8000],
        "files": files
    }

# ==========================================
# ğŸ§± åŸå­ç»„ä»¶ï¼šæµè§ˆå™¨æ“ä½œ (æ‹†åˆ†é™ç»´)
# ==========================================

def _init_browser_context(p):
    """åŸå­ä»»åŠ¡ï¼šå¯åŠ¨æµè§ˆå™¨å¹¶åŠ è½½çŠ¶æ€"""
    browser = p.chromium.launch(
        headless=False,
        args=['--disable-blink-features=AutomationControlled']
    )

    if os.path.exists(STATE_FILE):
        context = browser.new_context(storage_state=STATE_FILE, user_agent=DEFAULT_USER_AGENT)
    else:
        context = browser.new_context(user_agent=DEFAULT_USER_AGENT)

    context.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    return browser, context

def _navigate_and_fetch(page, url, context):
    """åŸå­ä»»åŠ¡ï¼šé¡µé¢å¯¼èˆªä¸æŠ“å–"""
    try:
        page.goto(url, timeout=90000, wait_until="domcontentloaded")
    except PlaywrightError as e:
        if "ERR_EMPTY_RESPONSE" in str(e) or "ERR_CONNECTION_RESET" in str(e):
            print(f"    âš ï¸ è¿æ¥è¢«åˆ‡æ–­ï¼ŒæŒ‡ç¤ºé‡è¯•...")
            return "RETRY"
        raise e

    page.wait_for_timeout(3000)

    if "404" in page.title() or "æŠ±æ­‰" in page.content():
        print("    âŒ é¡µé¢ 404")
        return "ABORT"

    if "login" in page.url:
        print("    âŒ Cookie/State å·²å¤±æ•ˆ")
        return "ABORT"

    html = page.content()
    fresh_cookies = _get_playwright_cookies(context)
    return _process_html(html, url, fresh_cookies)

def _perform_single_attempt(url):
    """æ‰§è¡Œå•æ¬¡æŠ“å–ä»»åŠ¡"""
    with sync_playwright() as p:
        browser, context = _init_browser_context(p)
        page = context.new_page()
        try:
            result = _navigate_and_fetch(page, url, context)
            return result
        except Exception as e:
            raise e
        finally:
            browser.close()

# ==========================================
# ğŸš€ ä¸»å…¥å£ (é‡æ„åå¤æ‚åº¦æä½)
# ==========================================

def fetch_content(url):
    """ä¸»è°ƒåº¦å‡½æ•°ï¼šåªè´Ÿè´£é‡è¯•é€»è¾‘"""
    MAX_RETRIES = 3

    for attempt in range(1, MAX_RETRIES + 1):
        try:
            if attempt > 1:
                wait_time = random.uniform(2, 5) * attempt
                print(f"    â³ ç½‘ç»œæ³¢åŠ¨ï¼Œç­‰å¾… {wait_time:.1f}s...")
                time.sleep(wait_time)

            result = _perform_single_attempt(url)

            if result == "ABORT":
                return None
            if result == "RETRY":
                continue
            if result:
                return result

        except Exception as e:
            print(f"    âŒ ç¬¬ {attempt} æ¬¡æŠ“å–å¤±è´¥: {e}")
            if attempt == MAX_RETRIES: return None

    return None