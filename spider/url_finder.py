import os
import json
import re
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
from urllib.parse import urljoin

class UrlFinder:
    def __init__(self):
        self.target_text = "ä¿¡æ¯å…¬å‘Š"
        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        self.cookie_file = os.path.join(base_dir, "data", "cookies.json")

    def find_new_urls(self, start_url):
        print(f"    ğŸ•·ï¸ [Finder] å¯åŠ¨... ç›®æ ‡é¦–é¡µ: {start_url}")

        all_candidates = []

        with sync_playwright() as p:
            # å¯åŠ¨æµè§ˆå™¨
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )

            # === 1. æ³¨å…¥ Cookie ===
            if os.path.exists(self.cookie_file):
                try:
                    with open(self.cookie_file, 'r', encoding='utf-8') as f:
                        cookies = json.load(f)
                        safe_cookies = []
                        for c in cookies:
                            if 'sameSite' in c and c['sameSite'] not in ['Strict', 'Lax', 'None', 'no_restriction']:
                                del c['sameSite']
                            safe_cookies.append(c)
                        context.add_cookies(safe_cookies)
                except:
                    pass

            page = context.new_page()

            try:
                # === 2. è®¿é—®é¦–é¡µ ===
                print(f"    ğŸ”— æ­£åœ¨è¿›å…¥ VPN é¦–é¡µ...")
                page.goto(start_url, timeout=60000)

                current_title = page.title()

                if "ç™»å½•" in current_title or "Login" in current_title or "ç”¨æˆ·ç™»å½•" in current_title:
                    print("    âŒ æ£€æµ‹åˆ° Cookie å¤±æ•ˆï¼")

                    # 1. ç«‹å³é”€æ¯è¿‡æœŸçš„ Cookie æ–‡ä»¶
                    if os.path.exists(self.cookie_file):
                        os.remove(self.cookie_file)
                        print("    ğŸ—‘ï¸ å·²è‡ªåŠ¨åˆ é™¤è¿‡æœŸ Cookie æ–‡ä»¶ã€‚")

                    # 2. è¿”å› None (è€Œä¸æ˜¯ç©ºåˆ—è¡¨)ï¼Œä½œä¸ºå‘ç»™ main.py çš„â€œé‡è¯•ä¿¡å·â€
                    return None

                # === 3. è¿›å…¥åˆ—è¡¨é¡µ ===
                print(f"    ğŸ” å¯»æ‰¾å¹¶ç‚¹å‡» [{self.target_text}]...")
                try:
                    with context.expect_page(timeout=15000) as new_page_info:
                        # å°è¯•ç²¾ç¡®åŒ¹é…æˆ–æ¨¡ç³ŠåŒ¹é…
                        page.get_by_text(self.target_text).first.click()

                    list_page = new_page_info.value
                    list_page.wait_for_load_state("domcontentloaded")

                    # ğŸ”´ å…³é”®ç‚¹ï¼šç­‰å¾… ul.news_list å‡ºç°ï¼Œè¿™æ˜¯æˆªå›¾é‡Œçš„æ ¸å¿ƒç‰¹å¾
                    try:
                        list_page.wait_for_selector("ul.news_list", timeout=5000)
                    except:
                        print("    âš ï¸ æœªæ‰¾åˆ°æ ‡å‡†åˆ—è¡¨ç»“æ„ï¼Œå°è¯•ç»§ç»­è§£æ...")

                    print("    ğŸ”€ æˆåŠŸè¿›å…¥å…¬å‘Šåˆ—è¡¨é¡µï¼")
                    base_url = list_page.url
                    html_content = list_page.content()
                    soup = BeautifulSoup(html_content, 'html.parser')

                    # === 4. è§£æåˆ—è¡¨ (åŸºäºæˆªå›¾ä¿®æ­£) ===
                    # æˆªå›¾æ˜¾ç¤ºï¼šul class="news_list clearfix" -> li class="news clearfix"

                    # 1. æ‰¾åˆ°æ‰€æœ‰çš„æ–°é—»é¡¹ (li æ ‡ç­¾)
                    # å…¼å®¹ news_list ä¸‹çš„ liï¼Œæˆ–è€…ç›´æ¥æ‰¾ class å« news çš„ li
                    items = soup.select("ul.news_list li")
                    if not items:
                        # å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥æ‰¾æ‰€æœ‰ class="news ..." çš„ li
                        items = soup.find_all("li", class_=lambda x: x and 'news' in x)

                    print(f"    ğŸ‘€ æ‰«æåˆ° {len(items)} æ¡æ•°æ® (åŸºäº ul/li ç»“æ„)...")

                    for item in items:
                        # --- A. æå–é“¾æ¥å’Œæ ‡é¢˜ ---
                        # é“¾æ¥é€šå¸¸åœ¨ a æ ‡ç­¾é‡Œ
                        link_tag = item.find('a', href=True)
                        if not link_tag: continue

                        title = link_tag.get_text(strip=True)
                        href = link_tag['href']

                        # --- B. æå–æ—¥æœŸ ---
                        # ç­–ç•¥1ï¼šæˆªå›¾æš—ç¤ºå¯èƒ½æœ‰ div class="title_sj" (sj=æ—¶é—´)
                        date_div = item.find(class_=re.compile("title_sj|date|time"))

                        date_str = "1970-01-01"

                        if date_div:
                            # å¦‚æœæ‰¾åˆ°äº†ä¸“é—¨æ”¾æ—¥æœŸçš„æ¡†
                            txt = date_div.get_text(strip=True)
                            match = re.search(r"(\d{4}-\d{2}-\d{2})", txt)
                            if match: date_str = match.group(1)
                        else:
                            # ç­–ç•¥2ï¼šæ²¡æ‰¾åˆ°ä¸“é—¨çš„classï¼Œå°±åœ¨æ•´ä¸ª li çš„æ–‡æœ¬é‡Œæ‰¾æ—¥æœŸ
                            # é™åˆ¶åªåœ¨è¿™ä¸ª li å†…éƒ¨æ‰¾ï¼Œç»å¯¹ä¸ä¼šè·¨è¡Œï¼
                            item_text = item.get_text(" ", strip=True)
                            match = re.search(r"(\d{4}-\d{2}-\d{2})", item_text)
                            if match: date_str = match.group(1)

                        # --- C. è¿‡æ»¤ ---
                        if len(title) < 4: continue
                        # è¿‡æ»¤æ‰éå…¬å‘Šçš„é“¾æ¥
                        blacklist = ["æ›´å¤š", "English", "é¦–é¡µ", "ä¸Šä¸€é¡µ", "å°¾é¡µ"]
                        if any(w in title for w in blacklist): continue

                        # ç»„è£…
                        full_url = urljoin(base_url, href)

                        all_candidates.append({
                            'url': full_url,
                            'title': title,
                            'date': date_str
                        })

                except Exception as e:
                    print(f"    âš ï¸ é¡µé¢æ“ä½œå¤±è´¥: {e}")

            except Exception as e:
                print(f"    âš ï¸ æŠ“å–è¿‡ç¨‹å¼‚å¸¸: {e}")
            finally:
                browser.close()

        # === 5. æ’åºä¸è¿”å› ===
        if all_candidates:
            # å†æ¬¡æŒ‰æ—¥æœŸå€’åº
            all_candidates.sort(key=lambda x: x['date'], reverse=True)

            print("    ğŸ“‰ [åˆ—è¡¨ç‰ˆ] æ’åºåçš„å‰ 5 æ¡å…¬å‘Š:")
            for idx, item in enumerate(all_candidates[:5]):
                print(f"       [{idx+1}] {item['date']} | {item['title'][:25]}...")

            return all_candidates[:5]

        return []