import os
import json
import re
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
from urllib.parse import urljoin

class UrlFinder:
    def __init__(self):
        self.target_text = "ä¿¡æ¯å…¬å‘Š"
        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        self.data_dir = os.path.join(base_dir, "data")
        self.cookie_file = os.path.join(self.data_dir, "cookies.json")
        self.state_file = os.path.join(self.data_dir, "state.json")

    def find_new_urls(self, start_url):
        """ä¸»å…¥å£"""
        print(f"    ğŸ•·ï¸ [Finder] å¯åŠ¨... ç›®æ ‡é¦–é¡µ: {start_url}")

        # ğŸŸ¢ ä¿®æ”¹1ï¼šæ¥æ”¶è¿”å›çš„ tuple (content, final_url)
        result = self._fetch_page_source(start_url)

        if result is None:
            return None

        html_content, final_url = result

        # ğŸŸ¢ ä¿®æ”¹2ï¼šä½¿ç”¨è·³è½¬åçš„ final_url ä½œä¸ºåŸºå‡†è¿›è¡Œæ‹¼æ¥
        print(f"    ğŸ“ åˆ—è¡¨é¡µçœŸå®åœ°å€: {final_url}")
        items = self._parse_html(html_content, final_url)

        if items:
            items.sort(key=lambda x: x['date'], reverse=True)
            print("    ğŸ“‰ [åˆ—è¡¨ç‰ˆ] æ’åºåçš„å‰ 5 æ¡å…¬å‘Š:")
            for idx, item in enumerate(items[:5]):
                print(f"       [{idx+1}] {item['date']} | {item['title'][:20]}...")
            return items[:5]
        return []

    # ==========================
    # ğŸ”§ åŸå­åŠŸèƒ½ç»„ä»¶
    # ==========================

    def _is_valid_link(self, href, text):
        if not href or href == '#' or 'javascript' in href.lower(): return False
        if text.startswith('[') and text.endswith(']'): return False
        ignore_words = {"æ›´å¤š", "è¯¦ç»†", "ç½®é¡¶", "new", "HOT", "é¦–é¡µ", "å°¾é¡µ"}
        if text in ignore_words: return False
        return True

    def _pick_best_link(self, candidates):
        if not candidates: return None
        for cand in candidates:
            if cand['len'] > 5: return cand['link']
        candidates.sort(key=lambda x: x['len'], reverse=True)
        return candidates[0]['link']

    def _extract_date(self, text):
        match = re.search(r"(\d{4}-\d{2}-\d{2})", text)
        return match.group(1) if match else "1970-01-01"

    # ==========================
    # ğŸ§± æµè§ˆå™¨æ“ä½œ
    # ==========================

    def _inject_cookies_fallback(self, context):
        if not os.path.exists(self.cookie_file): return
        try:
            with open(self.cookie_file, 'r', encoding='utf-8') as f:
                cookies = json.load(f)
                safe = [c for c in cookies if c.pop('sameSite', None) not in ['Strict', 'Lax']]
                context.add_cookies(safe)
        except: pass

    def _navigate_and_get_content(self, page, context):
        """
        é¡µé¢è·³è½¬é€»è¾‘
        è¿”å›: (html_content, current_url)
        """
        final_content = ""
        final_url = page.url

        # å¦‚æœå½“å‰é¡µå°±æœ‰ç›®æ ‡æŒ‰é’®ï¼Œå°è¯•ç‚¹å‡»
        if self.target_text in page.content():
            try:
                with context.expect_page(timeout=15000) as new_info:
                    page.get_by_text(self.target_text).first.click()

                list_page = new_info.value
                list_page.wait_for_load_state("domcontentloaded")

                # ç­‰å¾…åˆ—è¡¨åŠ è½½
                try: list_page.wait_for_selector("ul.news_list, tr", timeout=5000)
                except: pass

                final_content = list_page.content()
                final_url = list_page.url # ğŸŸ¢ æŠ“å–è·³è½¬åçš„ URL

            except:
                # ç‚¹å‡»å¤±è´¥ï¼Œå›é€€ä½¿ç”¨å½“å‰é¡µ
                final_content = page.content()
                final_url = page.url
        else:
            final_content = page.content()
            final_url = page.url

        return final_content, final_url

    def _fetch_page_source(self, url):
        """æµè§ˆå™¨ä¸»æµç¨‹"""
        result = None
        with sync_playwright() as p:
            # ä¿æŒä¹‹å‰çš„é…ç½® (æœ‰å¤´+éšèº«)
            browser = p.chromium.launch(
                headless=False,
                args=['--disable-blink-features=AutomationControlled']
            )

            user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

            if os.path.exists(self.state_file):
                # print(f"    ğŸ“‚ [Finder] åŠ è½½å®Œæ•´æµè§ˆå™¨çŠ¶æ€: {self.state_file}")
                context = browser.new_context(
                    storage_state=self.state_file,
                    user_agent=user_agent
                )
            else:
                print("    âš ï¸ æœªæ‰¾åˆ°çŠ¶æ€æ–‡ä»¶ï¼Œå°è¯•ä»…æ³¨å…¥ Cookie...")
                context = browser.new_context(user_agent=user_agent)
                self._inject_cookies_fallback(context)

            # æ³¨å…¥é˜²æ£€æµ‹
            context.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

            page = context.new_page()
            try:
                print(f"    ğŸ”— æ­£åœ¨è®¿é—®é¦–é¡µ...")
                page.goto(url, timeout=60000)

                if any(x in page.title() for x in ["ç™»å½•", "Login", "ç”¨æˆ·ç™»å½•"]):
                    print("    âŒ å‡­è¯å·²å¤±æ•ˆ (Redirected to Login)")
                    if os.path.exists(self.cookie_file): os.remove(self.cookie_file)
                    if os.path.exists(self.state_file): os.remove(self.state_file)
                    return None

                # ğŸŸ¢ ä¿®æ”¹3ï¼šè¿”å› (content, url)
                result = self._navigate_and_get_content(page, context)

            except Exception as e:
                print(f"    âš ï¸ æµè§ˆå™¨å¼‚å¸¸: {e}")
            finally:
                browser.close()
        return result

    # ==========================
    # ğŸ§µ æ ¸å¿ƒè§£æ
    # ==========================

    def _extract_link_from_row(self, row):
        all_links = row.find_all('a', href=True)
        if not all_links: return None
        valid_candidates = []
        for link in all_links:
            href = link['href'].strip()
            text = link.get_text(strip=True)
            if self._is_valid_link(href, text):
                valid_candidates.append({'link': link, 'len': len(text)})
        best_link = self._pick_best_link(valid_candidates)
        if not best_link: return None
        return {
            'url': best_link['href'],
            'title': best_link.get_text(strip=True),
            'date': self._extract_date(row.get_text(" ", strip=True))
        }

    def _parse_html(self, html, base_url):
        soup = BeautifulSoup(html, 'html.parser')
        candidates = []
        items = soup.select("ul.news_list li")
        if not items: items = soup.find_all("li", class_=lambda x: x and 'news' in x)
        if not items: items = soup.select("tr")

        print(f"    ğŸ‘€ æ‰«æåˆ° {len(items)} ä¸ªæ½œåœ¨è¡Œ...")

        for item in items:
            data = self._extract_link_from_row(item)
            if data:
                # ğŸŸ¢ è¿™é‡Œé€šè¿‡æ­£ç¡®çš„ base_url æ‹¼æ¥ï¼Œå°±ä¸ä¼š 404 äº†
                data['url'] = urljoin(base_url, data['url'])
                candidates.append(data)
        return candidates