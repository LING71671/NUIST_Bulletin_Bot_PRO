import os
import json
import re
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
from urllib.parse import urljoin

class UrlFinder:
    def __init__(self):
        self.target_text = "ä¿¡æ¯å…¬å‘Š"
        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        self.cookie_file = os.path.join(base_dir, "data", "cookies.json")

    def find_new_urls(self, start_url):
        """ä¸»å…¥å£"""
        print(f"    ğŸ•·ï¸ [Finder] å¯åŠ¨... ç›®æ ‡é¦–é¡µ: {start_url}")

        html_content = self._fetch_page_source(start_url)
        if html_content is None: return None

        items = self._parse_html(html_content, start_url)

        if items:
            items.sort(key=lambda x: x['date'], reverse=True)
            print("    ğŸ“‰ [åˆ—è¡¨ç‰ˆ] æ’åºåçš„å‰ 5 æ¡å…¬å‘Š:")
            for idx, item in enumerate(items[:5]):
                print(f"       [{idx+1}] {item['date']} | {item['title'][:20]}...")
            return items[:5]
        return []

    # ==========================
    # ğŸ”§ åŸå­åŠŸèƒ½ç»„ä»¶ï¼šé€»è¾‘åˆ¤æ–­
    # ==========================

    def _is_valid_link(self, href, text):
        """åˆ¤æ–­é“¾æ¥æ˜¯å¦æœ‰æ•ˆ"""
        if not href or href == '#' or 'javascript' in href.lower():
            return False
        # è¿‡æ»¤ [åˆ†ç±»]
        if text.startswith('[') and text.endswith(']'):
            return False
        # è¿‡æ»¤åŠŸèƒ½è¯
        ignore_words = {"æ›´å¤š", "è¯¦ç»†", "ç½®é¡¶", "new", "HOT", "é¦–é¡µ", "å°¾é¡µ"}
        if text in ignore_words:
            return False
        return True

    def _pick_best_link(self, candidates):
        """ä»ä¸€è¡Œä¸­é€‰å‡ºæœ€ä½³é“¾æ¥"""
        if not candidates: return None

        # ç­–ç•¥1ï¼šä¼˜å…ˆé€‰ç¬¬ä¸€ä¸ªé•¿åº¦ > 5 çš„ (é€šå¸¸æ˜¯æ ‡é¢˜)
        for cand in candidates:
            if cand['len'] > 5:
                return cand['link']

        # ç­–ç•¥2ï¼šå…œåº•é€‰æœ€é•¿çš„
        candidates.sort(key=lambda x: x['len'], reverse=True)
        return candidates[0]['link']

    def _extract_date(self, text):
        match = re.search(r"(\d{4}-\d{2}-\d{2})", text)
        return match.group(1) if match else "1970-01-01"

    # ==========================
    # ğŸ§± åŸå­åŠŸèƒ½ç»„ä»¶ï¼šæµè§ˆå™¨æ“ä½œ (æ‹†åˆ†è§£å†³å¤æ‚åº¦è­¦å‘Š)
    # ==========================

    def _inject_cookies(self, context):
        """ä»»åŠ¡ï¼šæ³¨å…¥Cookie"""
        if not os.path.exists(self.cookie_file): return
        try:
            with open(self.cookie_file, 'r', encoding='utf-8') as f:
                cookies = json.load(f)
                # æ¸…æ´— sameSite å­—æ®µ
                safe = [c for c in cookies if c.pop('sameSite', None) not in ['Strict', 'Lax']]
                context.add_cookies(safe)
        except Exception as e:
            print(f"    âš ï¸ Cookie è¯»å–å¾®ç‘•: {e}")

    def _navigate_and_get_content(self, page, context):
        """ä»»åŠ¡ï¼šå¤„ç†é¡µé¢è·³è½¬"""
        # å¦‚æœå½“å‰é¡µå°±æœ‰ç›®æ ‡æŒ‰é’®ï¼Œå°è¯•ç‚¹å‡»è·³è½¬
        if self.target_text in page.content():
            try:
                with context.expect_page(timeout=15000) as new_info:
                    page.get_by_text(self.target_text).first.click()
                list_page = new_info.value
                list_page.wait_for_load_state("domcontentloaded")
                try: list_page.wait_for_selector("ul.news_list, tr", timeout=5000)
                except: pass
                return list_page.content()
            except:
                # ç‚¹å‡»å¤±è´¥ï¼Œå›é€€ä½¿ç”¨å½“å‰é¡µ
                return page.content()
        return page.content()

    # ==========================
    # ğŸ§µ ä¸»æµç¨‹æ§åˆ¶
    # ==========================

    def _extract_link_from_row(self, row):
        """è§£æå•è¡Œæ•°æ®"""
        all_links = row.find_all('a', href=True)
        if not all_links: return None

        valid_candidates = []
        for link in all_links:
            href = link['href'].strip()
            text = link.get_text(strip=True)
            if self._is_valid_link(href, text):
                valid_candidates.append({'link': link, 'len': len(text)})

        best_link = self._pick_best_link(valid_candidates)
        if not best_link: return None

        return {
            'url': best_link['href'],
            'title': best_link.get_text(strip=True),
            'date': self._extract_date(row.get_text(" ", strip=True))
        }

    def _parse_html(self, html, base_url):
        """è§£ææ•´é¡µ HTML"""
        soup = BeautifulSoup(html, 'html.parser')
        candidates = []

        # å…¼å®¹å¤šç§åˆ—è¡¨é€‰æ‹©å™¨
        items = soup.select("ul.news_list li")
        if not items: items = soup.find_all("li", class_=lambda x: x and 'news' in x)
        if not items: items = soup.select("tr")

        print(f"    ğŸ‘€ æ‰«æåˆ° {len(items)} ä¸ªæ½œåœ¨è¡Œ...")

        for item in items:
            data = self._extract_link_from_row(item)
            if data:
                data['url'] = urljoin(base_url, data['url'])
                candidates.append(data)
        return candidates

    def _fetch_page_source(self, url):
        """
        æµè§ˆå™¨ä¸»æµç¨‹
        ç°åœ¨å®ƒåªæ˜¯ä¸€ä¸ªæŒ‡æŒ¥å®˜ï¼Œä¸è´Ÿè´£å…·ä½“å¹²æ´»ï¼Œå¤æ‚åº¦æä½
        """
        source = None
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=False)
            context = browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )

            # 1. æ³¨å…¥
            self._inject_cookies(context)

            page = context.new_page()
            try:
                print(f"    ğŸ”— æ­£åœ¨è®¿é—®é¦–é¡µ...")
                page.goto(url, timeout=60000)

                # 2. æ£€æŸ¥
                if any(x in page.title() for x in ["ç™»å½•", "Login", "ç”¨æˆ·ç™»å½•"]):
                    print("    âŒ Cookie å·²å¤±æ•ˆ")
                    if os.path.exists(self.cookie_file): os.remove(self.cookie_file)
                    return None

                # 3. è·³è½¬å¹¶è·å–
                source = self._navigate_and_get_content(page, context)

            except Exception as e:
                print(f"    âš ï¸ æµè§ˆå™¨å¼‚å¸¸: {e}")
            finally:
                browser.close()
        return source